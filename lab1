__author__ = 'lerxen'
import numpy as np
import math


def gradient_descent(vector, lyambda):
    e = 0.1
    result = np.zeros_like(vector)
    for i in range(len(vector)):
        result[i] = vector[i] - lyambda*(gradient(vector)[i])
    while (distance(vector, result) > e) \
            or (abs(rosenbrok_function(result) - rosenbrok_function(vector)) > e) \
            or norm(gradient(vector)) > e:
        vector = result
        for i in range(len(vector)):
            result[i] = vector[i] - lyambda*(gradient(vector)[i])
    return result


def gradient(vector):
    grade = np.zeros_like(vector)
    for i in range(len(vector)):
        if i == 0:
            grade[0] = -2*(1-vector[0]) - 400*vector[0]*(vector[1] - vector[0]**2)
        elif i == len(vector) - 1:
            grade[len(vector) - 1] = 200*(vector[len(vector)-1] - vector[len(vector)-2]**2)
        else:
            grade[i] = 200*(vector[i] - vector[i-1]**2) - 2*(1-vector[i]) - 400*vector[i]*(vector[i+1] - vector[i]**2)
    return grade


def rosenbrok_function(vector):
    f = 0
    for i in range(len(vector) - 1):
        f = f + (1 - vector[i])**2 + 100*(vector[i+1] - vector[i]**2)**2
    return f


def norm(vector):
    res = math.sqrt(np.sum(vector**2))
    return res


def distance(vector1, vector2):
    res = np.sum((vector1 - vector2)**2)
    return res


def decreasing_lyambda():
    lyambda = 1
    lyambda *= 0.0000001
    return lyambda


def optimal_step_lyambda(vector, n, delta):
    vector2 = np.zeros_like(vector)
    e = 2**64
    for i in range(n):
        delta *= np.random.uniform(0.01, 0.02)
        for k in range(len(vector)):
            vector2[k] = vector[k] - delta*(gradient(vector)[k])
        if rosenbrok_function(vector) < e:
            e = rosenbrok_function(vector2)
            lyambda = delta
    return lyambda


def Monte_Carlo_method(n, vectelemnumb, low, high):
    randvector1 = np.random.randint(low, high, vectelemnumb)
    for i in range(n):
        randvector2 = np.random.randint(low, high, vectelemnumb)
        if rosenbrok_function(randvector2) < rosenbrok_function(randvector1):
            randvector1 = randvector2
    return randvector1


x = np.array([0.5, 0.5, 0.2, 0.5, 0.1], dtype='float')
print(gradient_descent(x, 0.001))

